{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from codonUtils import utils\n",
    "from codonOptimizer import tableOptimizer\n",
    "from codonTable import codonTable\n",
    "import bct\n",
    "import pickle\n",
    "import subprocess\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class tableOptimizer in module codonOptimizer:\n",
      "\n",
      "class tableOptimizer(builtins.object)\n",
      " |  A class designed to optimize a codon table given an arbitrary objective \n",
      " |  function to minimize/maximize\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  GDA(self, dW=None, W=0, W_stop=inf, maxIter=1000, preserveBlock=False, preserveStop=False, subFunc='minPRS')\n",
      " |      The Great Deluge Algorithm for optimizing an objective function over\n",
      " |      codon table space\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      - float dW=None: represents the rate of change (rain flux) of min\n",
      " |          allowable energy (water level); if None, initialized to 1% of\n",
      " |          cost(self.table)\n",
      " |      - float W=0: represents the initial minimal acceptable energy level\n",
      " |          (water level); defaults to 0\n",
      " |      - float W_stop=inf: represents the maximum allowable water level; stops\n",
      " |          algorithm when W > W_stop; defaults to an infinte value\n",
      " |      - int maxIter=1000: represents the maximum number of iterations to\n",
      " |          perform; defaults to 1000\n",
      " |      - bool preserveBlock=False: a bool that tells the function whether or\n",
      " |          not to preserve block structure when shuffling the table\n",
      " |      - bool preserveStop=False: a bool that tells the function whether or not\n",
      " |          to shuffle blocks encoding for STOP\n",
      " |      - str subFunc: an optional parameter specifying the substitution metric\n",
      " |          to be used in maximization. Defaults to __sub_minPRS().\n",
      " |      \n",
      " |          Acceptable inputs:\n",
      " |          - 'minPRS' --> __sub_minPRS()\n",
      " |          - 'Gilis' --> __sub_Gilis()\n",
      " |          - 'SCV' --> __sub_SCV()\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      - dict table: a python dict representing an optimized codon table\n",
      " |      - np.array Ws: a numpy array representing water level vs iteration\n",
      " |      - np.array Es: a numpy array representing cost(table) vs iteration\n",
      " |  \n",
      " |  __init__(self, table={'GCU': 'A', 'UCC': 'S', 'CAC': 'H', 'CAG': 'Q', 'AAG': 'K', 'UUU': 'F', 'CCU': 'P', 'GUG': 'V', 'CCA': 'P', 'CUC': 'L', 'GAG': 'E', 'GGA': 'G', 'CAU': 'H', 'AAC': 'N', 'UUG': 'L', 'UAC': 'Y', 'UGG': 'W', 'UCG': 'S', 'GAA': 'E', 'CGG': 'R', 'AGG': 'R', 'AUU': 'I', 'UAU': 'Y', 'CGC': 'R', 'UUA': 'L', 'AAA': 'K', 'GUA': 'V', 'ACG': 'T', 'CAA': 'Q', 'UAA': '*', 'CCG': 'P', 'GGG': 'G', 'AGU': 'S', 'AUC': 'I', 'CGU': 'R', 'ACU': 'T', 'GAU': 'D', 'GCG': 'A', 'CUA': 'L', 'UGA': '*', 'AGC': 'S', 'CGA': 'R', 'GUC': 'V', 'AAU': 'N', 'GUU': 'V', 'GCC': 'A', 'GCA': 'A', 'ACC': 'T', 'AUA': 'I', 'UCA': 'S', 'GGU': 'G', 'UGU': 'C', 'CUG': 'L', 'ACA': 'T', 'UCU': 'S', 'AUG': 'M', 'UGC': 'C', 'UAG': '*', 'CUU': 'L', 'UUC': 'F', 'GAC': 'D', 'CCC': 'P', 'GGC': 'G', 'AGA': 'R'}, costfxn=None, wobble_rule='standard', debug=False)\n",
      " |      the init function for the tableOptimizer class. Optionally allows the\n",
      " |      user to specify the starting codon table, associated objective\n",
      " |      function, and which wobble_rules should be followed. Default values are\n",
      " |      supplied if the user does not specify them (i.e. Standard Code, 47\n",
      " |      block wobble rule)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      - dict table = utils.standardTable: a python dict representing the\n",
      " |          codon table. Also optionally accepts a codonTable object.\n",
      " |      - func costfxn = None: a function that takes a python dict as an\n",
      " |          input and outputs a cost associated with that table. If no\n",
      " |          funciton is supplied, defaults to maxMutMinVar.\n",
      " |      - str wobble_rule = 'standard': a string telling the simulator which\n",
      " |          wobble rules to follow for accepting new tables\n",
      " |      \n",
      " |          Acceptable inputs:\n",
      " |          - 'standard' : 48 blocks, 2 stop blocks\n",
      " |          - 'preserveBlock' : maintain same block structure as standard\n",
      " |              table\n",
      " |          - 'unrestricted' : 63 open blocks, at least 0 of every AA and\n",
      " |              stop.\n",
      " |      \n",
      " |      - bool debug: optional boolean telling the initializer whether or not\n",
      " |          to run self.__debug()\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tableOptimizer obj: returns an instance of the tableOptimizer object\n",
      " |  \n",
      " |  tableShuffle(self, table, preserveBlock=False, preserveStop=False)\n",
      " |      Takes a codon table as an input and shuffles it to produce a new,\n",
      " |          similar table in order to traverse table space.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      - dict table: a python dict representing a codon table starting point\n",
      " |      - bool preserveBlock=False: a bool that tells the function whether or\n",
      " |          not to preserve block structure when shuffling the table\n",
      " |      - bool preserveStop=False: a bool that tells the function whether or not\n",
      " |          to shuffle blocks encoding for STOP\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict newTable: a python dict representing the next codon table\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  maxMutMinVar(table, subFunc)\n",
      " |      the default cost function for class. Implements a version of the\n",
      " |      cost function used by Novozhilov et al. 2007, but optimizes for\n",
      " |      maximizing mutability while minimizing variance per mutation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      - dict table: a python dictionary representing the codon table to\n",
      " |          evaluate\n",
      " |      - func subFunc(str AA_1, str AA_2): a function representing the penalty\n",
      " |          for substituting two amino acids\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float metric: the absolute number score of the given table\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tableOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Statistics\n",
    "\n",
    "A set of adjacency matrices that preserve the degree, weight and strength distributions of the Standard Codon table graph were generated using bct.null_model_dir_sign() and used as a null model. A set of evolved tables were also generated and the densities of the resulting sets were compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# define number of trials to run\n",
    "N = 1000\n",
    "sim = tableOptimizer()\n",
    "# preallocate memory for statistics\n",
    "densities = np.zeros(N)\n",
    "efficiencies = np.zeros(N)\n",
    "clustCoeff = np.zeros(N)\n",
    "modularities = np.zeros(N)\n",
    "assortativities = np.zeros(N)\n",
    "silencicities = np.zeros(N)\n",
    "# perform N trials\n",
    "for i in range(N):\n",
    "    # generage graph\n",
    "    ct, *dummy = sim.GDA(subFunc='Gilis')\n",
    "    ct = codonTable(ct)\n",
    "    Cij = ct.codonAdjMat\n",
    "    # calculate statistics\n",
    "    densities[i], *dummy = bct.density_und(Cij)\n",
    "    efficiencies[i] = bct.efficiency_wei(Cij)\n",
    "    clustCoeff[i], *dummy = bct.clustering_coef_wu(Cij)\n",
    "    dummy, modularities[i] = bct.modularity_und(Cij)\n",
    "    assortativities[i] = bct.assortativity_wei(Cij, 0)\n",
    "    silencicities[i] = utils.silencicity(ct.codonDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle resulting statistics so they don't have to be re calculated\n",
    "stats = [densities, efficiencies, clustCoeff, modularities, assortativities, silencicities]\n",
    "with open('res/Gilis.pickle', 'wb') as handle:\n",
    "    pickle.dump(stats, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Models\n",
    "The null models used for this analysis preserve the in/out degree and strength of the standard codon table, but shuffle the edges. This allows for a direct comparison of any generated tables vs a set of \"standard tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/.local/lib/python3.5/site-packages/numpy/lib/function_base.py:3162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n"
     ]
    }
   ],
   "source": [
    "## Null Models\n",
    "%autoreload 2\n",
    "# define number of adjMats to generate\n",
    "N = 10000\n",
    "# get standard table\n",
    "ct = codonTable()\n",
    "C_st = ct.codonAdjMat\n",
    "# preallocate memory for statistics\n",
    "null_densities = np.zeros(N)\n",
    "null_efficiencies = np.zeros(N)\n",
    "null_clustCoeff = np.zeros(N)\n",
    "null_modularities = np.zeros(N)\n",
    "null_assortativities = np.zeros(N)\n",
    "# perform N trials\n",
    "for i in range(N):\n",
    "    # generate new null graph\n",
    "    Cij, *dummy = bct.null_model_und_sign(C_st)\n",
    "    # calculate statistics\n",
    "    null_densities[i], *dummy = bct.density_und(Cij)\n",
    "    null_efficiencies[i] = bct.efficiency_wei(Cij)\n",
    "    null_clustCoeff[i], *dummy = bct.clustering_coef_wu(Cij)\n",
    "    dummy, null_modularities[i] = bct.modularity_und(Cij)\n",
    "    null_assortativities[i] = bct.assortativity_wei(Cij, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle resulting statistics so they don't have to be re calculated\n",
    "stats = [null_densities, null_efficiencies, null_clustCoeff, null_modularities, null_assortativities]\n",
    "with open('res/null.pickle', 'wb') as handle:\n",
    "    pickle.dump(stats, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tables\n",
    "A set of tables were generated randomly by using the most permissible biological block structure and maintaining at least one block per residue plus stop. This set of tables was analyzed as a benchmark against which the standard table can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## random Tables\n",
    "%autoreload 2\n",
    "# define number of trials to run\n",
    "N = 100000\n",
    "sim = tableOptimizer()\n",
    "# preallocate memory for statistics\n",
    "densities = np.zeros(N)\n",
    "efficiencies = np.zeros(N)\n",
    "clustCoeff = np.zeros(N)\n",
    "modularities = np.zeros(N)\n",
    "assortativities = np.zeros(N)\n",
    "silencicities = np.zeros(N)\n",
    "# perform N trials\n",
    "for i in range(N):\n",
    "    # generage graph\n",
    "    ct = utils.randomTable()\n",
    "    ct = codonTable(ct)\n",
    "    Cij = ct.codonAdjMat\n",
    "    # calculate statistics\n",
    "    densities[i], *dummy = bct.density_und(Cij)\n",
    "    efficiencies[i] = bct.efficiency_wei(Cij)\n",
    "    clustCoeff[i], *dummy = bct.clustering_coef_wu(Cij)\n",
    "    dummy, modularities[i] = bct.modularity_und(Cij)\n",
    "    assortativities[i] = bct.assortativity_wei(Cij, 0)\n",
    "    silencicities[i] = utils.silencicity(ct.codonDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle resulting statistics so they don't have to be re calculated\n",
    "stats = [densities, efficiencies, clustCoeff, modularities, assortativities, silencicities]\n",
    "with open('res/rand.pickle', 'wb') as handle:\n",
    "    pickle.dump(stats, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualize the data!!\n",
    "\n",
    "# unpickle data\n",
    "with open('res/minPRS.pickle', 'rb') as handle:\n",
    "    PRS_densities, PRS_efficiencies, PRS_clustCoeff, PRS_modularities, PRS_assortativities, PRS_silencicities = pickle.load(handle)\n",
    "\n",
    "with open('res/Gilis.pickle', 'rb') as handle:\n",
    "    Gilis_densities, Gilis_efficiencies, Gilis_clustCoeff, Gilis_modularities, Gilis_assortativities, Gilis_silencicities = pickle.load(handle)\n",
    "\n",
    "with open('res/SCV.pickle', 'rb') as handle:\n",
    "    SCV_densities, SCV_efficiencies, SCV_clustCoeff, SCV_modularities, SCV_assortativities, SCV_silencicities = pickle.load(handle)\n",
    "\n",
    "with open('res/null.pickle', 'rb') as handle:\n",
    "    null_densities, null_efficiencies, null_clustCoeff, null_modularities, null_assortativities = pickle.load(handle)\n",
    "\n",
    "with open('res/rand.pickle', 'rb') as handle:\n",
    "    rand_densities, rand_efficiencies, rand_clustCoeff, rand_modularities, rand_assortativities, rand_silencicities = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "fig, axarr = plt.subplots(5, sharex=True)\n",
    "null_weights = np.ones_like(null_densities)/float(len(null_densities))\n",
    "rand_weights = np.ones_like(rand_densities)/float(len(rand_densities))\n",
    "evo_weights = np.ones_like(PRS_densities)/float(len(PRS_densities))\n",
    "axarr[0].hist(null_densities, 20, weights=null_weights, color='grey', alpha=0.5, label='Null')\n",
    "axarr[1].hist(rand_densities, 20, weights=rand_weights, color='black', alpha=0.5, label='Random')\n",
    "axarr[2].hist(PRS_densities, 20, weights=evo_weights, color='red', alpha=0.5, label='PRS')\n",
    "axarr[3].hist(Gilis_densities, 20, weights=evo_weights, color='green', alpha=0.5, label='Gilis')\n",
    "axarr[4].hist(SCV_densities, 20, weights=evo_weights, color='blue', alpha=0.5, label='SCV')\n",
    "fig.suptitle('Histogram of Densities')\n",
    "for i in range(5):\n",
    "    h, l = axarr[i].get_legend_handles_labels()\n",
    "    axarr[i].legend(h, l)\n",
    "    axarr[i].set_ylim(0, 1)\n",
    "plt.xlabel('Density')\n",
    "plt.savefig('../figs/density_stats.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "## do statistics ##\n",
    "###################\n",
    "\n",
    "# test if distributions are identical to null\n",
    "D_dens_rand, p_dens_rand = stats.ks_2samp(rand_densities, null_densities)\n",
    "D_dens_PRS, p_dens_PRS = stats.ks_2samp(PRS_densities, null_densities)\n",
    "D_dens_Gilis, p_dens_Gilis = stats.ks_2samp(Gilis_densities, null_densities)\n",
    "D_dens_SCV, p_dens_SCV = stats.ks_2samp(SCV_densities, null_densities)\n",
    "# test if means are different\n",
    "Z_dens_rand, U_dens_rand = stats.ranksums(rand_densities, null_densities)\n",
    "Z_dens_PRS, U_dens_PRS = stats.ranksums(PRS_densities, null_densities)\n",
    "Z_dens_Gilis, U_dens_Gilis = stats.ranksums(Gilis_densities, null_densities)\n",
    "Z_dens_SCV, U_dens_SCV = stats.ranksums(SCV_densities, null_densities)\n",
    "# package into pandas dataframe for representation\n",
    "index = ['KS Statistic', 'KS p-Val','Wilcoxon Z Statistic', 'Wilcoxon U-Val']\n",
    "dict = {\n",
    "    'Randomly Generated' : [D_dens_rand, p_dens_rand, Z_dens_rand, U_dens_rand],\n",
    "    'PRS' : [D_dens_PRS, p_dens_PRS, Z_dens_PRS, U_dens_PRS],\n",
    "    'Gilis' : [D_dens_Gilis, p_dens_Gilis, Z_dens_Gilis, U_dens_Gilis],\n",
    "    'SCV' : [D_dens_SCV, p_dens_SCV, Z_dens_SCV, U_dens_SCV]\n",
    "}\n",
    "dens_stats = pd.DataFrame(dict, index=index)\n",
    "dens_stats.to_html('res/dens_stats.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "fig, axarr = plt.subplots(5, sharex=True)\n",
    "null_weights = np.ones_like(null_efficiencies)/float(len(null_efficiencies))\n",
    "rand_weights = np.ones_like(rand_efficiencies)/float(len(rand_efficiencies))\n",
    "evo_weights = np.ones_like(PRS_efficiencies)/float(len(PRS_efficiencies))\n",
    "axarr[0].hist(null_efficiencies, 50, weights=null_weights, color='grey', alpha=0.5, label='Null')\n",
    "axarr[1].hist(rand_efficiencies, 50, weights=rand_weights, color='black', alpha=0.5, label='Random')\n",
    "axarr[2].hist(PRS_efficiencies, 50, weights=evo_weights, color='red', alpha=0.5, label='PRS')\n",
    "axarr[3].hist(Gilis_efficiencies, 50, weights=evo_weights, color='green', alpha=0.5, label='Gilis')\n",
    "axarr[4].hist(SCV_efficiencies, 50, weights=evo_weights, color='blue', alpha=0.5, label='SCV')\n",
    "fig.suptitle('Histogram of Efficiencies')\n",
    "for i in range(5):\n",
    "    h, l = axarr[i].get_legend_handles_labels()\n",
    "    axarr[i].legend(h, l)\n",
    "    axarr[i].set_ylim(0, 0.1)\n",
    "plt.xlabel('Efficiency')\n",
    "plt.savefig('../figs/efficiency_stats.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "## do statistics ##\n",
    "###################\n",
    "\n",
    "# test if distributions are identical to null\n",
    "D_eff_rand, p_eff_rand = stats.ks_2samp(rand_efficiencies, null_efficiencies)\n",
    "D_eff_PRS, p_eff_PRS = stats.ks_2samp(PRS_efficiencies, null_efficiencies)\n",
    "D_eff_Gilis, p_eff_Gilis = stats.ks_2samp(Gilis_efficiencies, null_efficiencies)\n",
    "D_eff_SCV, p_eff_SCV = stats.ks_2samp(SCV_efficiencies, null_efficiencies)\n",
    "# test if means are different\n",
    "Z_eff_rand, U_eff_rand = stats.ranksums(rand_efficiencies, null_efficiencies)\n",
    "Z_eff_PRS, U_eff_PRS = stats.ranksums(PRS_efficiencies, null_efficiencies)\n",
    "Z_eff_Gilis, U_eff_Gilis = stats.ranksums(Gilis_efficiencies, null_efficiencies)\n",
    "Z_eff_SCV, U_eff_SCV = stats.ranksums(SCV_efficiencies, null_efficiencies)\n",
    "# package into pandas dataframe for representation\n",
    "index = ['KS Statistic', 'KS p-Val','Wilcoxon Z Statistic', 'Wilcoxon U-Val']\n",
    "dict = {\n",
    "    'Randomly Generated' : [D_eff_rand, p_eff_rand, Z_eff_rand, U_eff_rand],\n",
    "    'PRS' : [D_eff_PRS, p_eff_PRS, Z_eff_PRS, U_eff_PRS],\n",
    "    'Gilis' : [D_eff_Gilis, p_eff_Gilis, Z_eff_Gilis, U_eff_Gilis],\n",
    "    'SCV' : [D_eff_SCV, p_eff_SCV, Z_eff_SCV, U_eff_SCV]\n",
    "}\n",
    "eff_stats = pd.DataFrame(dict, index=index)\n",
    "eff_stats.to_html('res/eff_stats.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "fig, axarr = plt.subplots(5, sharex=True)\n",
    "null_weights = np.ones_like(null_clustCoeff)/float(len(null_clustCoeff))\n",
    "rand_weights = np.ones_like(rand_clustCoeff)/float(len(rand_clustCoeff))\n",
    "evo_weights = np.ones_like(PRS_clustCoeff)/float(len(PRS_clustCoeff))\n",
    "axarr[0].hist(null_clustCoeff, 50, weights=null_weights, color='grey', alpha=0.5, label='Null')\n",
    "axarr[1].hist(rand_clustCoeff, 50, weights=rand_weights, color='black', alpha=0.5, label='Random')\n",
    "axarr[2].hist(PRS_clustCoeff, 50, weights=evo_weights, color='red', alpha=0.5, label='PRS')\n",
    "axarr[3].hist(Gilis_clustCoeff, 50, weights=evo_weights, color='green', alpha=0.5, label='Gilis')\n",
    "axarr[4].hist(SCV_clustCoeff, 50, weights=evo_weights, color='blue', alpha=0.5, label='SCV')\n",
    "fig.suptitle('Histogram of Clustering Coefficients')\n",
    "for i in range(5):\n",
    "    h, l = axarr[i].get_legend_handles_labels()\n",
    "    axarr[i].legend(h, l)\n",
    "    axarr[i].set_ylim(0, 0.1)\n",
    "plt.xlabel('Clustering Coefficient')\n",
    "#plt.savefig('../figs/density_stats.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "## do statistics ##\n",
    "###################\n",
    "\n",
    "# test if distributions are identical to null\n",
    "D_clustCoeff_rand, p_clustCoeff_rand = stats.ks_2samp(rand_clustCoeff, null_clustCoeff)\n",
    "D_clustCoeff_PRS, p_clustCoeff_PRS = stats.ks_2samp(PRS_clustCoeff, null_clustCoeff)\n",
    "D_clustCoeff_Gilis, p_clustCoeff_Gilis = stats.ks_2samp(Gilis_clustCoeff, null_clustCoeff)\n",
    "D_clustCoeff_SCV, p_clustCoeff_SCV = stats.ks_2samp(SCV_clustCoeff, null_clustCoeff)\n",
    "# test if means are different\n",
    "Z_clustCoeff_rand, U_clustCoeff_rand = stats.ranksums(rand_clustCoeff, null_clustCoeff)\n",
    "Z_clustCoeff_PRS, U_clustCoeff_PRS = stats.ranksums(PRS_clustCoeff, null_clustCoeff)\n",
    "Z_clustCoeff_Gilis, U_clustCoeff_Gilis = stats.ranksums(Gilis_clustCoeff, null_clustCoeff)\n",
    "Z_clustCoeff_SCV, U_clustCoeff_SCV = stats.ranksums(SCV_clustCoeff, null_clustCoeff)\n",
    "# package into pandas dataframe for representation\n",
    "index = ['KS Statistic', 'KS p-Val','Wilcoxon Z Statistic', 'Wilcoxon U-Val']\n",
    "dict = {\n",
    "    'Randomly Generated' : [D_clustCoeff_rand, p_clustCoeff_rand, Z_clustCoeff_rand, U_clustCoeff_rand],\n",
    "    'PRS' : [D_clustCoeff_PRS, p_clustCoeff_PRS, Z_clustCoeff_PRS, U_clustCoeff_PRS],\n",
    "    'Gilis' : [D_clustCoeff_Gilis, p_clustCoeff_Gilis, Z_clustCoeff_Gilis, U_clustCoeff_Gilis],\n",
    "    'SCV' : [D_clustCoeff_SCV, p_clustCoeff_SCV, Z_clustCoeff_SCV, U_clustCoeff_SCV]\n",
    "}\n",
    "clustCoeff_stats = pd.DataFrame(dict, index=index)\n",
    "clustCoeff_stats.to_html('res/clustCoeff_stats.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "fig, axarr = plt.subplots(5, sharex=True)\n",
    "null_weights = np.ones_like(null_modularities)/float(len(null_modularities))\n",
    "rand_weights = np.ones_like(rand_modularities)/float(len(rand_modularities))\n",
    "evo_weights = np.ones_like(PRS_modularities)/float(len(PRS_modularities))\n",
    "axarr[0].hist(null_modularities, 50, weights=null_weights, color='grey', alpha=0.5, label='Null')\n",
    "axarr[1].hist(rand_modularities, 50, weights=rand_weights, color='black', alpha=0.5, label='Random')\n",
    "axarr[2].hist(PRS_modularities, 50, weights=evo_weights, color='red', alpha=0.5, label='PRS')\n",
    "axarr[3].hist(Gilis_modularities, 50, weights=evo_weights, color='green', alpha=0.5, label='Gilis')\n",
    "axarr[4].hist(SCV_modularities, 50, weights=evo_weights, color='blue', alpha=0.5, label='SCV')\n",
    "fig.suptitle('Histogram of Modularity metrics')\n",
    "for i in range(5):\n",
    "    h, l = axarr[i].get_legend_handles_labels()\n",
    "    axarr[i].legend(h, l)\n",
    "    axarr[i].set_ylim(0, 0.1)\n",
    "plt.xlabel('Modularity Metric')\n",
    "#plt.savefig('../figs/density_stats.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "## do statistics ##\n",
    "###################\n",
    "\n",
    "# test if distributions are identical to null\n",
    "D_mod_rand, p_mod_rand = stats.ks_2samp(rand_modularities, null_modularities)\n",
    "D_mod_PRS, p_mod_PRS = stats.ks_2samp(PRS_modularities, null_modularities)\n",
    "D_mod_Gilis, p_mod_Gilis = stats.ks_2samp(Gilis_modularities, null_modularities)\n",
    "D_mod_SCV, p_mod_SCV = stats.ks_2samp(SCV_modularities, null_modularities)\n",
    "# test if means are different\n",
    "Z_mod_rand, U_mod_rand = stats.ranksums(rand_modularities, null_modularities)\n",
    "Z_mod_PRS, U_mod_PRS = stats.ranksums(PRS_modularities, null_modularities)\n",
    "Z_mod_Gilis, U_mod_Gilis = stats.ranksums(Gilis_modularities, null_modularities)\n",
    "Z_mod_SCV, U_mod_SCV = stats.ranksums(SCV_modularities, null_modularities)\n",
    "# package into pandas dataframe for representation\n",
    "index = ['KS Statistic', 'KS p-Val','Wilcoxon Z Statistic', 'Wilcoxon U-Val']\n",
    "dict = {\n",
    "    'Randomly Generated' : [D_mod_rand, p_mod_rand, Z_mod_rand, U_mod_rand],\n",
    "    'PRS' : [D_mod_PRS, p_mod_PRS, Z_mod_PRS, U_mod_PRS],\n",
    "    'Gilis' : [D_mod_Gilis, p_mod_Gilis, Z_mod_Gilis, U_mod_Gilis],\n",
    "    'SCV' : [D_mod_SCV, p_mod_SCV, Z_mod_SCV, U_mod_SCV]\n",
    "}\n",
    "mod_stats = pd.DataFrame(dict, index=index)\n",
    "mod_stats.to_html('res/mod_stats.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assortativities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "fig, axarr = plt.subplots(5, sharex=True)\n",
    "null_weights = np.ones_like(null_assortativities)/float(len(null_assortativities))\n",
    "rand_weights = np.ones_like(rand_assortativities)/float(len(rand_assortativities))\n",
    "evo_weights = np.ones_like(PRS_assortativities)/float(len(PRS_assortativities))\n",
    "axarr[0].hist(null_assortativities, 50, weights=null_weights, color='grey', alpha=0.5, label='Null')\n",
    "axarr[1].hist(rand_assortativities, 50, weights=rand_weights, color='black', alpha=0.5, label='Random')\n",
    "axarr[2].hist(PRS_assortativities, 50, weights=evo_weights, color='red', alpha=0.5, label='PRS')\n",
    "axarr[3].hist(Gilis_assortativities, 50, weights=evo_weights, color='green', alpha=0.5, label='Gilis')\n",
    "axarr[4].hist(SCV_assortativities, 50, weights=evo_weights, color='blue', alpha=0.5, label='SCV')\n",
    "fig.suptitle('Histogram of Assortativities')\n",
    "for i in range(5):\n",
    "    h, l = axarr[i].get_legend_handles_labels()\n",
    "    axarr[i].legend(h, l)\n",
    "    axarr[i].set_ylim(0, 0.15)\n",
    "plt.xlabel('Assortativity')\n",
    "#plt.savefig('../figs/assortativity_stats.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "## do statistics ##\n",
    "###################\n",
    "\n",
    "# test if distributions are identical to null\n",
    "D_assort_rand, p_assort_rand = stats.ks_2samp(rand_assortativities, null_assortativities)\n",
    "D_assort_PRS, p_assort_PRS = stats.ks_2samp(PRS_assortativities, null_assortativities)\n",
    "D_assort_Gilis, p_assort_Gilis = stats.ks_2samp(Gilis_assortativities, null_assortativities)\n",
    "D_assort_SCV, p_assort_SCV = stats.ks_2samp(SCV_assortativities, null_assortativities)\n",
    "# test if means are different\n",
    "Z_assort_rand, U_assort_rand = stats.ranksums(rand_assortativities, null_assortativities)\n",
    "Z_assort_PRS, U_assort_PRS = stats.ranksums(PRS_assortativities, null_assortativities)\n",
    "Z_assort_Gilis, U_assort_Gilis = stats.ranksums(Gilis_assortativities, null_assortativities)\n",
    "Z_assort_SCV, U_assort_SCV = stats.ranksums(SCV_assortativities, null_assortativities)\n",
    "# package into pandas dataframe for representation\n",
    "index = ['KS Statistic', 'KS p-Val','Wilcoxon Z Statistic', 'Wilcoxon U-Val']\n",
    "dict = {\n",
    "    'Randomly Generated' : [D_assort_rand, p_assort_rand, Z_assort_rand, U_assort_rand],\n",
    "    'PRS' : [D_assort_PRS, p_assort_PRS, Z_assort_PRS, U_assort_PRS],\n",
    "    'Gilis' : [D_assort_Gilis, p_assort_Gilis, Z_assort_Gilis, U_assort_Gilis],\n",
    "    'SCV' : [D_assort_SCV, p_assort_SCV, Z_assort_SCV, U_assort_SCV]\n",
    "}\n",
    "assort_stats = pd.DataFrame(dict, index=index)\n",
    "assort_stats.to_html('res/assort_stats.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silencicity\n",
    "\n",
    "Silencicity is a lab defined metric, specifying the fraction of mutations that are synonymous. Code was written that generates random codon tables, specifying the block structure, and a set of these tables were evaluated for silencicity. The Standard Table was compared to the ensemble of random tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "rand_weights = np.ones_like(rand_silencicities)/float(len(rand_silencicities))\n",
    "n, bins, patches = plt.hist(rand_silencicities, 30, weights=rand_weights, color='grey', alpha=0.5)\n",
    "plt.suptitle('Silencicities for Random Tables')\n",
    "plt.xlabel('Silencicity')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.yscale('log')\n",
    "#plt.savefig('../figs/silencicity_stats.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "ct = codonTable()\n",
    "stSilencicity = utils.silencicity(ct.codonTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stSilencicity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
